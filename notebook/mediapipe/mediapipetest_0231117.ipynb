{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_hands=mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "image = cv2.imread(r\"D:\\project_meta\\NNproject\\NNI\\output\\video_frames\\IDFolder\\1\\1\\1_150.jpg\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(image_rgb)\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "cv2.namedWindow('Hand Detection', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Hand Detection', 800, 600)\n",
    "cv2.imshow('Hand Detection', image)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "image = cv2.imread(r\"D:\\project_meta\\NNproject\\NNI\\output\\video_frames\\IDFolder\\1\\1\\1_150.jpg\")\n",
    "image = cv2.imread(r\"D:\\project_meta\\NNproject\\NNI\\output\\video_frames\\IDFolder\\1\\1\\1_150.jpg\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = face_mesh.process(image_rgb)\n",
    "if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "             mp_drawing.draw_landmarks(image, hand_landmarks, mp_face_mesh.)\n",
    "cv2.imshow('FaceMesh', frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# 初始化 FaceMesh 模型\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "# 读取图像文件\n",
    "image = cv2.imread(r\"D:\\project_meta\\NNproject\\NNI\\output\\video_frames\\IDFolder\\1\\1\\1_4050.jpg\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 进行人脸关键点识别\n",
    "results = face_mesh.process(image_rgb)\n",
    "\n",
    "# 绘制人脸关键点\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "         mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                # landmark_drawing_spec为关键点可视化样式，None为默认样式（不显示关键点）\n",
    "                # landmark_drawing_spec=mp_drawing_styles.DrawingSpec(thickness=1,circle_radius=2,color=[66,77,229]),\n",
    "                landmark_drawing_spec=None,\n",
    "                # connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "            )\n",
    "\n",
    "# 显示结果\n",
    "cv2.namedWindow('Face Detection', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Face Detection', 800, 600)\n",
    "cv2.imshow('Face Detection', image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nose tip:\n",
      "x: 0.5540912747383118\n",
      "y: 0.4317862391471863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 对于静态图像\n",
    "IMAGE_FILES = [r\"D:\\project_meta\\NNproject\\NNI\\output\\video_frames\\IDFolder\\1\\1\\1_4050.jpg\"]\n",
    "with mp_face_detection.FaceDetection() as face_detection:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # 转换BGR图像到RGB和使用MediaPipe人脸检测处理它\n",
    "    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 绘制人脸检测到每个人脸\n",
    "    if not results.detections:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for detection in results.detections:\n",
    "        \"\"\"The enum type of the six face detection key points.\n",
    "        RIGHT_EYE = 0\n",
    "        LEFT_EYE = 1\n",
    "        NOSE_TIP = 2\n",
    "        MOUTH_CENTER = 3\n",
    "        RIGHT_EAR_TRAGION = 4\n",
    "        LEFT_EAR_TRAGION = 5\n",
    "        \"\"\"\n",
    "        print('Nose tip:')\n",
    "        print(mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "        # Nose tip:\n",
    "        # x: 0.3519737124443054\n",
    "        # y: 0.4148605167865753\n",
    "        mp_drawing.draw_detection(annotated_image, detection)\n",
    "    cv2.imwrite('annotated_image' + str(idx) + '.png', annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[label_id: 0\n",
       " score: 0.6294022798538208\n",
       " location_data {\n",
       "   format: RELATIVE_BOUNDING_BOX\n",
       "   relative_bounding_box {\n",
       "     xmin: 0.4768142104148865\n",
       "     ymin: 0.3258226811885834\n",
       "     width: 0.16622394323349\n",
       "     height: 0.2007233202457428\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5349845886230469\n",
       "     y: 0.38988155126571655\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5889388918876648\n",
       "     y: 0.4005175828933716\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5540912747383118\n",
       "     y: 0.4317862391471863\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5511273741722107\n",
       "     y: 0.46633946895599365\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5062620639801025\n",
       "     y: 0.3910195231437683\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.6303325295448303\n",
       "     y: 0.41243791580200195\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "def new_func(imagePath):\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# 加载图像\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "# 使用Face Detection模块\n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "    # 处理检测结果\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "            # 获取人脸位置和置信度\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x, y, w, h = int(bbox.xmin * image.shape[1]), int(bbox.ymin * image.shape[0]), \\\n",
    "                         int(bbox.width * image.shape[1]), int(bbox.height * image.shape[0])\n",
    "                confidence = detection.score\n",
    "\n",
    "            # 绘制边界框\n",
    "            # cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            \n",
    "            # 截取人脸\n",
    "                face_image = image[y-20:y+h, x-10:x+w+10]\n",
    "    return face_image\n",
    "face_image=new_func(r\"D:\\project_meta\\NNproject\\NNI\\output\\video_frames\\IDFolder\\1\\1\\1_4050.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
